import numpy as np
import os
from sklearn.model_selection import StratifiedGroupKFold
import mlflow
import logging
import torch
from datetime import datetime
import ray
from ray import train, tune
from ray.air.integrations.mlflow import MLflowLoggerCallback, setup_mlflow
from ray.tune.schedulers import AsyncHyperBandScheduler
import multiprocessing

from dataset import MEGGraphs
from data_utils import *
from train import *
from visualize_graphs import *
from utils import *
from plot_graph_analysis import *
from experiment_utils import *

'''
GNN model for MEG data classification of stimulation ON and OFF states.
Created by: Caroline Witstok, 2025

This script is the main entry point for training and testing a GNN model on MEG data for classifcation of stimulation ON and OFF.

Before running this script, make sure to have the necessary dependencies installed and the data available in the specified directories (see README file).
1. Specify the state of the run (training or testing) by setting the `run_state` variable. If applicable, specify the cross analysis parameters.
2. Specify whether plots should be generated by setting the `generate_plots` variable to True or False.
3. Specify whether explainability analysis should be performed by setting the `explainability_analysis` variable to True or False.
4. Specify the input type by setting the `input_type` variable to either 'fif' or 'scout'.
5. Activate MLflow tracking in command line by running: mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns --host 0.0.0.0 --port 5000
6. Check the 'Raw' and 'Processed' folders, and change the data files and graphs as needed.
'''

# Specificy the state of this run (training and saving a new model or testing, using a saved model)
run_state = 'training'  # 'training' or 'testing'
if run_state == 'testing':
    # For performing cross analysis, define the cross model name and data to use as input
    run_cross_analysis = True           # Set to True to run saved model on new data, False to skip
    cross_model_name = "_TONIC_only"    # The model name for the previously trained model to load
    new_data_analysis = "BURST"         # The data to use as input

# Specifiy the model name
# model_name = '_SCOUT_PTPTNall_TONIC_BURST_PSD_1_100Hz_30dur_25overl' # '_PTPTNall_BURST_TONIC_baseline_PSD_negatives_1_100Hz_30dur_25overl' # 

# Specify whether plots should be generated when running (set to True), or not (set to False)
generate_plots = True

# Specify whether explainability analysis should be performed (set to True), or not (set to False)
# Note: this analysis is only performed when a saved model is tested, not when a new model is trained
explainability_analysis = True

# Specify the input type ('fif' for sensor analysis, or 'scout' for source analysis)
input_type = 'scout'


def create_dataset(analysis_name='Full', input_type='fif', fif_folder=None, scout_folder=None):
    '''
    Calls the MEGGraphs class (see dataset.py) to create a dataset of graphs out of raw MEG data.
    The inputs needed for the MEGGraphs class are defined here.

    INPUTS:
        - analysis_name : Name of the analysis to be performed
        - input_type    : Type of input data ('fif' or 'scout')
        - fif_folder    : Folder containing the fif data (if None, the data will be loaded from the default location)
        - scout_folder  : Folder containing the scout data (if None, the data will be loaded from the default location)
    OUTPUTS:
        - dataset       : Dataset of graphs
        - duration      : Duration of subepochs in seconds
        - overlap       : Overlap of subepochs in seconds
        - num_graphs    : Number of graphs in the dataset
        - fmax          : Maximum frequency of the computed PSD
    '''

    if input_type == 'fif':
        # Define filenames of MEG data exported as fif file
        directory = fif_folder if fif_folder is not None else r'F:\MEG_GNN\GNN\Data\Raw'
        filenames = [f for f in os.listdir(directory) if f.endswith('.fif')]
        filenames = sorted(filenames)

        # Define scouts data list as None
        scouts_data_list = None

    elif input_type == 'scout':
        # Define filenames of MEG data exported as scout time series data
        directory = scout_folder if scout_folder is not None else r'F:\MEG_GNN\GNN\Data\Raw\Scout_files'
        fif_directory = fif_folder if fif_folder is not None else r'F:\MEG_GNN\GNN\Data\Raw'
        scout_filenames = [f for f in os.listdir(directory) if f.endswith('.h5')]
        scout_filenames = sorted(scout_filenames)
        
        # Retrieve list of dictionaries containing the scouts data for each file
        scouts_data_list = load_scouts_data(scout_filenames, directory)

        # Retrieve list of raw fif filenames corresponding to the scout data files
        filenames = get_raw_file(scout_filenames, fif_directory)

    else:
        print(f'Input type {input_type} not recognized')

    # Define path to stimulation information Excel file
    stim_excel_file = r'F:\MEG_GNN\MEG_data\MEG_PT_notes.xlsx'

    # Retrieve stimulation info for each file and store it in a dictionary
    stim_info_dict = {}
    for filename in filenames:
        stim_info = get_stimulation_info(stim_excel_file, filename)
        if stim_info is not None:
            print(f"Stimulation info found for {filename}:")
        else:
            print(f"No stimulation info found or enterred for {filename}")
        stim_info_dict[filename] = stim_info

    # Define the duration of the subepochs that are created out of the epochs and the amount of overlap between them (in seconds)
    duration = 30
    overlap = 25

    # Define the ramp time (in seconds) for the subepochs to account for ramping of stimulation effects
    ramp_time = 5

    # Define minimum and maximum frequency for PSD calculation (maximum is resample_freq/2 due to Nyquist theorem; 128 Hz in this case)
    frequency_bands = {
        "full":   (1, 100),
        "delta":  (1, 4),
        "theta":  (4, 8),
        "alpha":  (8, 12),
        "theta_alpha": (4, 12),
        "beta":   (12, 30),
        "gamma":  (30, 100),
    }
    fmin, fmax = frequency_bands.get(analysis_name.lower(), (1, 100))  # Default to full band if analysis name not recognized

    # Define connectivity method for defining edges
    conn_method = 'pli'

    # Define the frequency resolution for the PSD and PLI calculation (in Hz, default = 1 Hz)
    freq_res = 1

    # Define root directory to the 'raw' and 'processed' folders that store the MEG data and the graphs, respectively
    root_directory = r'F:\MEG_GNN\GNN\Data'
    
    # Call the MEGGraphs class (see dataset.py)
    if input_type == 'fif':
        dataset = MEGGraphs(input_type=input_type,
                            root=root_directory,
                            filenames=filenames,
                            stim_info_dict=stim_info_dict, 
                            duration=duration,
                            overlap=overlap,
                            conn_method=conn_method,
                            fmin=fmin,
                            fmax=fmax,
                            ramp_time=ramp_time,
                            conn_save_dir=f'F:\MEG_GNN\GNN\Data\Connectivity\Subepoch_{duration}sec_{overlap}overlap_freq_{fmin}_{fmax}Hz_freqres_{freq_res}Hz',
                            freq_res=freq_res
        )
    # If the input is scout time series data, add the scouts_data_list argument
    elif input_type == 'scout':
        dataset = MEGGraphs(input_type=input_type,
                            root=root_directory,
                            filenames=filenames,
                            stim_info_dict=stim_info_dict, 
                            duration=duration,
                            overlap=overlap,
                            conn_method=conn_method,
                            fmin=fmin,
                            fmax=fmax,
                            ramp_time=ramp_time,
                            conn_save_dir=f'F:\MEG_GNN\GNN\Data\Connectivity\Subepoch_scouts_{duration}sec_{overlap}overlap_freq_{fmin}_{fmax}Hz_freqres_{freq_res}Hz',
                            freq_res=freq_res,
                            scout_data_list=scouts_data_list  # Only added if input_type is 'scout'
        )

    print('dataset', dataset)
    print('total graphs per file', dataset.graphs_per_file())
    print('total stim ON graphs per file', dataset.stim_graphs_per_file())
    print('total stim OFF graphs per file', dataset.non_stim_graphs_per_file())

    # Retrieve total number of graphs in the dataset
    num_graphs = len(dataset)

    return dataset, duration, overlap, num_graphs, fmax, scouts_data_list

def split_train_test(dataset):
    '''
    Splits the Dataset object into a train and test set using Stratified Group split.
    The split is based on the patient IDs to ensure that all graphs of a patient are in either the train or test set.
    The labels are used to ensure that the split is stratified, meaning that the distribution of labels is preserved in both sets.

    INPUT:
        - dataset           : Dataset of graphs

    OUTPUTS: 
        - dataset_train     : Dataset of graphs for training
        - dataset_test      : Dataset of graphs for testing 
        - y_train           : list of labels of train set 
        - y_test            : list of labels of test set

    '''

    # Get the patient IDs and labels from the dataset
    labels = [data.y.item() for data in dataset if data.y is not None]
    patient_ids = [data.patient_id for data in dataset if data.y is not None]
    dataset = [data for data in dataset if data.y is not None]

    # Apply Stratified Group K-Fold split and acquire train and test indices
    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=1)
    train_idx, test_idx = next(sgkf.split(np.zeros(len(labels)), labels, groups=patient_ids))

    # Create train and test datasets based on the indices
    dataset_train = [dataset[i] for i in train_idx]
    dataset_test = [dataset[i] for i in test_idx]
    y_train = [labels[i] for i in train_idx]
    y_test = [labels[i] for i in test_idx]

    return dataset_train, dataset_test, y_train, y_test

def short_trial_dirname_creator(trial):
    '''
    Create a short name for the trial based on its parameters. This is used to define the directory where the results
    of the hyperparameter tuning are stored.

    INPUTS:
        - trial     : Trial object

    OUTPUTS:
        - string    : Short name for the trial
    '''

    params = trial.config
    return f"trial_{trial.trial_id[:8]}_n_layers={params['n_layers']}_lr={params['lr']}_bs={params['batch_size']}"

def train_hyperparameters(dataset, dataset_train, y_train, model_name=None):
    '''
    Trains the GNN model (see model.py) using the Ray trainable train_func (see train.py).
    The search space used for the hyperparameter tuning is defined here.

    INPUTS: 
        - dataset           : Dataset of graphs
        - dataset_train     : Dataset of graphs for training 
        - y_train           : list of labels of train set
        - model_name        : Name of the model to be saved (if None, a new model will be trained)
    
    OUTPUTS: 
        - results           : ResultGrid object of results of all hyperparameter configurations
        - best_result       : Result object of results of best hyperparameter configuration
        - best_params       : dictionary of the best hyperparameter configuration
    '''

    print('Setting up MLflow URI...')
    # Set the MLflow tracking URI
    os.environ["MLFLOW_TRACKING_URI"] = "http://localhost:5000"
    
    print('Initializing Ray...')
    # Terminate processes started by ray.init(), so you can define a local _temp_dir to store the Ray process files
    if ray.is_initialized():
        ray.shutdown()
        print('Ray shutdown.')

 
    ray.init(_temp_dir=r"F:\MEG_GNN\GNN\Ray_temp", logging_level=logging.ERROR)
    print('Ray initialized.')

    print('Initilizing MLflow logging...')
    # Initialize MLflow logging
    experiment_name = f"mlflow_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    mlflow.set_experiment(experiment_name)
    mlflow_tracking_uri = os.getenv("MLFLOW_TRACKING_URI")
    print('MLflow tracking URI:', mlflow_tracking_uri)

    # Make sure Ray doesn't change the working directory to the trial directory, so you can define your own (relative) path to store results 
    os.environ["RAY_CHDIR_TO_TRIAL_DIR"] = "0"

    # Make sure Ray can handle reporting more than one metric 
    os.environ["TUNE_DISABLE_STRICT_METRIC_CHECKING"] = "1"

    # Define hyperparameter search space
    search_space = {
        'n_layers': tune.choice([2, 3, 4]),
        'dropout_rate': tune.choice([0.01, 0.1, 0.3, 0.5]),
        'conv1_hidden_channels': tune.choice([16, 32, 64, 128]),
        'lr': tune.choice([0.00001, 0.0001, 0.001, 0.01]),
        'batch_size': tune.choice([2, 4, 8, 16, 32, 64, 128]),
        'weight_decay': tune.choice([0.00001, 0.0001, 0.001]),
        'edge_filtering': tune.choice(['Threshold_TopK', 'MST']),
        'top_k': tune.choice([None, 300, 600, 900, 1200]),
        'threshold': tune.choice([None, 0.01, 0.03, 0.05, 0.07])
    }

    # Define the scheduler for hyperparameter tuning
    scheduler = AsyncHyperBandScheduler(
        time_attr="training_iteration",
        max_t=100,
        grace_period=25,
        reduction_factor=3
    )

    # Define path where results need to be stored
    run_config = train.RunConfig(
        name = 'tune_hyperparameters',
        storage_path=r"F:\MEG GNN\GNN\Ray_results",
        callbacks=[
            MLflowLoggerCallback(
                tracking_uri=mlflow_tracking_uri,
                experiment_name=experiment_name,
                save_artifact=True,
            )
        ],
    )

    # Define how Ray should choose the 'best_results'
    tune_config = tune.TuneConfig(
        metric='val_accuracy',
        mode='max',
        num_samples=20,
        scheduler=scheduler,
        trial_dirname_creator=short_trial_dirname_creator,
    )

    # Perform the training and hyperparameter tuning
    tuner = tune.Tuner(
        tune.with_parameters(train_func, dataset=dataset, dataset_train=dataset_train, y_train=y_train, model_name=model_name),
        param_space=search_space,
        tune_config=tune_config,
        run_config=run_config
    )
    results = tuner.fit()

    # Retrieve best result
    best_result = results.get_best_result()

    # Retrieve hyperparameter configuration of best result
    best_params = best_result.config

    # Save the best result and hyperparameters
    output_dir = os.path.join(os.path.dirname(__file__), 'Output')
    os.makedirs(output_dir, exist_ok=True)
    best_result_path = os.path.join(output_dir, f"best_result{model_name}.pt")
    torch.save({
        'best_result': best_result,
        'best_params': best_params
    }, best_result_path)

    return results, best_result, best_params

def load_best_result(best_result_path):
    '''
    Loads the saved best result and its configuration parameters.

    INPUT:
        - best_result_path : Path to the saved best result file
    
    OUTPUT:
        - best_result      : Loaded best result object
        - best_params      : Loaded configuration parameters
    '''

    checkpoint = torch.load(best_result_path, weights_only=False)
    best_result = checkpoint['best_result']
    best_params = checkpoint['best_params']
    print('best result', best_result)
    print('best params', best_params)

    return best_result, best_params

def filter_dataset(dataset_train, dataset_test, best_result):
    '''
    Apply edge filtering using TopK filtering and threshold filtering to the train and test dataset with
    the best performing hyperparameters.

    INPUTS:
        - dataset_train             : Dataset list of graphs for training
        - dataset_test              : Dataset list of graphs for testing
        - best_result               : Result object of results of best hyperparameter configuration

    OUTPUTS:
        - filtered_dataset_train    : Dataset list of graphs for training with filtered edges
        - filtered_dataset_test     : Dataset list of graphs for testing with filtered edges
    '''

    # Retrieve hyperparameter configuration for the TopK and threshold values of the best performing result
    top_k = best_result.config['top_k']
    threshold = best_result.config['threshold']

    # Apply edge filtering to the train and test dataset
    filtered_dataset_train = dataset_train.copy()
    filtered_dataset_train = [edge_filtering(graph, top_k, threshold) for graph in filtered_dataset_train]
    filtered_dataset_test = dataset_test.copy()
    filtered_dataset_test = [edge_filtering(graph, top_k, threshold) for graph in filtered_dataset_test]

    return filtered_dataset_train, filtered_dataset_test

def filter_entire_dataset(dataset, best_result):
    '''
    Apply edge filtering using TopK filtering and threshold filtering to the entire dataset with
    the best performing hyperparameters.

    INPUTS:
        - dataset                   : Dataset of graphs
        - best_result               : Result object of results of best hyperparameter configuration

    OUTPUTS:
        - filtered_dataset_complete : Dataset of graphs for testing a saved model (not split in train and test set)
    '''

    # Retrieve hyperparameter configuration for the TopK and threshold values of the best performing result
    top_k = best_result.config['top_k']
    threshold = best_result.config['threshold']

    # Convert dataset to a list of graphs
    graph_list = [dataset[i] for i in range(len(dataset))]

    # Apply edge filtering to the entire dataset
    filtered_dataset_complete = [edge_filtering(graph, top_k, threshold) for graph in graph_list]

    # Retrieve the number of edges in each graph
    num_edges = [graph.edge_index.shape[1] for graph in filtered_dataset_complete]

    return filtered_dataset_complete, num_edges

def main(run_state, analysis_name, model_name, generate_plots, explainability_analysis, input_type='fif', fif_folder=None, scout_folder=None):
    '''
    Calls all functions in this script.

    INPUT:
        - run_state:                Run state of the experiment ('training' or 'testing')
        - analysis_name:            Name of the analysis to be performed
        - model_name:               Name of the model to be saved (if None, a new model will be trained)
        - generate_plots:           Whether to generate plots of the dataset and results (True or False)
        - explainability_analysis:  Whether to perform explainability analysis on the model (True or False)
        - input_type:               Type of input data ('fif' or 'scout')
        - fif_folder:               Folder containing the fif data (if None, the data will be loaded from the default location)
        - scout_folder:             Folder containing the scout data (if None, the data will be loaded from the default location)
    OUTPUT: N/A
    '''

    # Create dataset of graphs
    dataset, duration, overlap, num_graphs, fmax, scouts_data_list = create_dataset(analysis_name, input_type=input_type, fif_folder=fif_folder, scout_folder=scout_folder)

    # Remove bad graphs from the dataset after the dataset has been created
    dataset.remove_bad_graphs()
    print('dataset', len(dataset))

    # Archive the processed graphs if not already archived to 'Processed_saved' folder
    archive_dir = os.path.join(r"F:\MEG_GNN\GNN\Data\Processed_saved", analysis_name.upper())
    processed_dir = r"F:\MEG_GNN\GNN\Data\Processed"
    if os.path.exists(archive_dir):
        shutil.rmtree(archive_dir)
    shutil.copytree(processed_dir, archive_dir)
    print(f"Archived processed graphs to {archive_dir}")

    # Create a dedicated output directory for this model
    output_dir = os.path.join(os.path.dirname(__file__), 'Output', model_name)
    os.makedirs(output_dir, exist_ok=True)

    # Check the state of the run (training or testing) and specify a path to save or load the model
    if run_state == 'training':
        # If a new model is trained, the model path is None, and a new model will be saved to the Output folder
        model_path = None
    elif run_state == 'testing':
        # If a saved model is tested, the model path is specified
        model_path = os.path.join(output_dir, f'best_model{model_name}.pt')
    else:
        print(f'State {run_state} is not configured correctly to either training or testing.')
    print('Model path:', model_path)

    # If generate_plots = True, plot and visualize the dataset
    if generate_plots:
        # Define output directory for plots
        plot_output_dir = os.path.join(output_dir, f'Plots{model_name}')
        # Filter out graphs without labels
        graphs = [graph for graph in dataset if graph.y is not None]

        # Plot graphs if input type is 'fif'
        if scouts_data_list is None:
            node_positions = load_meg_channels()
            node_positions_dict = {i: pos for i, pos in enumerate(node_positions.values())}
            print('node position dict', node_positions_dict)
            node_labels_dict = {i: label for i, label in enumerate(node_positions.keys())}
            for label in ['stimulation ON', 'stimulation OFF']:
                plot_average_psd(graphs, label, plot_output_dir, scout_coords_df=None, node_labels=list(node_labels_dict.values()))
                plot_selected_node_features(graphs, plot_output_dir, fmax=fmax, num_nodes=3, node_labels=node_labels_dict)
                plot_combined_average_psd(graphs, plot_output_dir, fmax=fmax)
                avg_conn_matrix_on = plot_connectivity_matrix(graphs, 'stimulation ON', plot_output_dir, node_labels=list(node_labels_dict.values()))
                avg_conn_matrix_off = plot_connectivity_matrix(graphs, 'stimulation OFF', plot_output_dir, node_labels=list(node_labels_dict.values()))
                plot_connectivity_matrix_difference(avg_conn_matrix_on, avg_conn_matrix_off, plot_output_dir, node_labels=list(node_labels_dict.values()))

        # Plot graphs if input type is 'scout'               
        else:
            node_positions, scout_coords_df = load_scout_names()
            node_positions_dict = {i: pos for i, pos in enumerate(node_positions.values())}
            print('node position dict', node_positions_dict)
            node_labels_dict = {i: label for i, label in enumerate(node_positions.keys())}
            for label in ['stimulation ON', 'stimulation OFF']:
                plot_average_psd(graphs, label, plot_output_dir, scout_coords_df=scout_coords_df, node_labels=list(node_labels_dict.values()))
                plot_selected_node_features(graphs, plot_output_dir, fmax=fmax, num_nodes=3, node_labels=node_labels_dict)
                plot_combined_average_psd(graphs, plot_output_dir, fmax=fmax)
                avg_conn_matrix_on = plot_connectivity_matrix(graphs, 'stimulation ON', plot_output_dir, node_labels=list(node_labels_dict.values()), scout_coords_df=scout_coords_df)
                avg_conn_matrix_off = plot_connectivity_matrix(graphs, 'stimulation OFF', plot_output_dir, node_labels=list(node_labels_dict.values()), scout_coords_df=scout_coords_df)
                plot_connectivity_matrix_difference(avg_conn_matrix_on, avg_conn_matrix_off, plot_output_dir, node_labels=list(node_labels_dict.values()), scout_coords_df=scout_coords_df)

    if model_path is None:
        # Prepare labels and groups for splitting with Stratified Group K-Fold
        labels = [data.y.item() for data in dataset if data.y is not None]
        patient_ids = [data.patient_id for data in dataset if data.y is not None]
        dataset_list = [data for data in dataset if data.y is not None]

        # Specify the splitter for Stratified Group K-Fold using 5 folds
        sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=1)
        all_metrics = []
        all_conf_matrices = []
        all_y_true = []
        all_y_prob = []

        # Perform 5 fold cross-validation with Stratified Group K-Fold
        for fold, (train_idx, test_idx) in enumerate(sgkf.split(np.zeros(len(labels)), labels, groups=patient_ids)):
            print(f"Fold {fold+1}/5")
            dataset_train = [dataset_list[i] for i in train_idx]
            dataset_test = [dataset_list[i] for i in test_idx]
            y_train = [labels[i] for i in train_idx]
            y_test = [labels[i] for i in test_idx]

            # Train and tune hyperparameters
            results, best_result, best_params = train_hyperparameters(dataset, dataset_train, y_train, model_name=f"{model_name}_fold{fold+1}")

            # Filter the dataset using the selected edge filtering method
            if best_result.config['edge_filtering'] == 'Threshold_TopK':
                filtered_dataset_train, filtered_dataset_test = filter_dataset(dataset_train, dataset_test, best_result)
            elif best_result.config['edge_filtering'] == 'MST':
                filtered_dataset_train = [mst_filtering(graph) for graph in dataset_train]
                filtered_dataset_test = [mst_filtering(graph) for graph in dataset_test]

            # Evaluate on test set
            acc_test, f1, roc_auc, precision, recall, conf_matrix, y_true, y_prob = test_best_model(
                best_result, dataset, filtered_dataset_test, f"{model_name}_fold{fold+1}", output_dir
            )
            all_conf_matrices.append(conf_matrix)
            all_y_true.extend(y_true)
            all_y_prob.extend(y_prob)
            print(f'Fold {fold+1} Test accuracy: {acc_test}')

            # Collect metrics
            all_metrics.append({
                "accuracy": acc_test,
                "f1": f1,
                "roc_auc": roc_auc,
                "precision": precision,
                "recall": recall,
                "conf_matrix": conf_matrix
            })

        # After all folds, print average metrics
        mean_accuracy = np.mean([m["accuracy"] for m in all_metrics])
        mean_f1 = np.mean([m["f1"] for m in all_metrics])
        mean_roc_auc = np.mean([m["roc_auc"] for m in all_metrics])
        print(f"\nMean Accuracy over 5 folds: {mean_accuracy:.3f}")
        print(f"Mean F1 over 5 folds: {mean_f1:.3f}")
        print(f"Mean ROC AUC over 5 folds: {mean_roc_auc:.3f}")

        # Create a Confusion matrix across all folds
        total_conf_matrix = np.sum(all_conf_matrices, axis=0)

        # Create ROC curve across all folds
        fpr, tpr, thresholds = roc_curve(all_y_true, all_y_prob, pos_label=1)
        roc_auc = auc(fpr, tpr)
        roc_data = pd.DataFrame({
            'False Positive Rate': fpr,
            'True Positive Rate': tpr,
            'Thresholds': thresholds
        })
        roc_data['Model'] = model_name

        # Save performance metrics across all folds to text file
        metrics = {
            "Test Accuracy": mean_accuracy,
            "F1 Score": mean_f1,
            "ROC AUC": mean_roc_auc,
            "Confusion Matrix": total_conf_matrix.tolist(),
        }
        save_metrics_to_txt(metrics, dataset.get_filenames(), output_directory, f"train_test{model_name}.txt")

        # Save ROC curve plot and data to CSV
        plt.figure()
        plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Receiver Operating Characteristic (ROC) Curve (All Folds)')
        plt.legend(loc="lower right")
        plt.savefig(os.path.join(plot_output_dir, f'ROC_curve_all_folds.png'))
        plt.close()
        roc_data.to_csv(os.path.join(output_dir, 'ROC', f'ROC_curve{model_name}.csv'), index=False)

    elif model_path is not None:
        print('Loading saved model...')
        # Load the saved best result and hyperparameters
        best_result_path = os.path.join(os.path.dirname(__file__), 'Output', f'best_result{model_name}.pt')
        best_result, best_params = load_best_result(best_result_path)
        print('Best result:', best_result)
        print('Best params:', best_params)

        # Filter the entire dataset using the selected edge filtering method
        if best_result.config['edge_filtering'] == 'Threshold_TopK':
            # Filter the entire dataset using the threshold and top_k values of the best performing result
            filtered_dataset_complete, num_edges = filter_entire_dataset(dataset, best_result)
        elif best_result.config['edge_filtering'] == 'MST':
            # Filter the entire dataset using the Maximum Spanning Tree (MST) method
            filtered_dataset_complete = [mst_filtering(graph) for graph in dataset]

        # Load and test a saved model on the current dataset
        acc_test, f1, roc_auc, precision, recall, conf_matrix = load_and_test_model(model_path, best_result, dataset, filtered_dataset_complete)
        print(f'Test accuracy: {acc_test}')

        # Save metrics to text file
        metrics = {
            "Test Accuracy": acc_test,
            "F1 Score": f1,
            "ROC AUC": roc_auc,
            "Precision": precision,
            "Recall": recall,
            "Confusion Matrix": conf_matrix.tolist()
        }
        output_directory = os.path.join(os.path.dirname(__file__), 'Output', f'Test_results{model_name}')
        save_metrics_to_txt(metrics, dataset.get_filenames(), output_directory, f"test{model_name}.txt")

        if explainability_analysis:
            # Apply explainability to the model using the SubgraphX method
            print('Applying explainability...')
            explain_file = f"explain{model_name}"
            print('explain file', explain_file)
            explain_dir = f"Explainability output/{explain_file}_{input_type}"
            os.makedirs(explain_dir, exist_ok=True)
            explain_model(best_result, filtered_dataset_complete, dataset, input_type, explain_dir)

def run_saved_model_on_new_data(new_data_analysis, model_name, input_type='fif', generate_plots=False, explainability_analysis=False):
    """
    Loads a saved model (trained on saved_model_analysis) and evaluates it on new_data_analysis input data.

    INPUTS:
        - new_data_analysis:        The new data to be evaluated.
        - model_name:               The name of the model that is previously trained and saved.
        - input_type:               The type of input data ('fif' for sensor analysis, or 'scout' for source analysis).
        - generate_plots:           Whether to generate plots of the dataset and results (True or False).
        - explainability_analysis:  Whether to perform explainability analysis on the model (True or False).
    OUTPUTS:
        N/A
    """

    # Prepare the new input data
    fif_folder, scout_folder = fix_raw_files(new_data_analysis, input_type=input_type)
    dataset, duration, overlap, num_graphs, fmax, scouts_data_list = create_dataset(
        new_data_analysis, input_type=input_type, fif_folder=fif_folder, scout_folder=scout_folder
    )
    dataset.remove_bad_graphs()

    # Load the saved best result and hyperparameters for the saved_model_analysis
    output_dir = os.path.join(os.path.dirname(__file__), 'Output', model_name)
    best_result_path = os.path.join(os.path.dirname(__file__), 'Output', f'best_result{model_name}.pt')
    best_result, best_params = load_best_result(best_result_path)
    print('Best result:', best_result)
    print('Best params:', best_params)

    # Filter the new dataset using the selected edge filtering method
    if best_result.config['edge_filtering'] == 'Threshold_TopK':
        filtered_dataset_complete, num_edges = filter_entire_dataset(dataset, best_result)
    elif best_result.config['edge_filtering'] == 'MST':
        filtered_dataset_complete = [mst_filtering(graph) for graph in dataset]

    # Load and test the saved model on the new dataset
    model_path = os.path.join(output_dir, f'best_model{model_name}.pt')
    acc_test, f1, roc_auc, precision, recall, conf_matrix = load_and_test_model(
        model_path, best_result, dataset, filtered_dataset_complete
    )
    print(f'Test accuracy on {new_data_analysis}: {acc_test}')

    # Save metrics to text file
    metrics = {
        "Test Accuracy": acc_test,
        "F1 Score": f1,
        "ROC AUC": roc_auc,
        "Precision": precision,
        "Recall": recall,
        "Confusion Matrix": conf_matrix.tolist()
    }
    output_directory = os.path.join(os.path.dirname(__file__), 'Output', f'Test_results_{model_name}_on_{new_data_analysis}')
    save_metrics_to_txt(metrics, dataset.get_filenames(), output_directory, f"test_{model_name}_on_{new_data_analysis}.txt")

    # Perform explainability analysis of the new data on the saved model, if specified
    if explainability_analysis:
        print('Applying explainability...')
        explain_file = f"explain_model{model_name}_on_{new_data_analysis}"
        explain_dir = f"Explainability output/{explain_file}_{input_type}"
        os.makedirs(explain_dir, exist_ok=True)
        explain_model(best_result, filtered_dataset_complete, dataset, input_type, explain_dir)

if __name__ == "__main__":
    '''
    Runs the main function. 
    '''
    
    multiprocessing.set_start_method('spawn', force=True)
    
    # Define the analyses to be performed with their corresponding model names
    analyses = [
        {
            "analysis_name": "BURST",
            "model_name": "_BURST_only",
        },
        {
            "analysis_name": "TONIC",
            "model_name": "_TONIC_only",
        },
        {
            "analysis_name": "Canada",
            "model_name": "_Canada_only",
        },
        {
            "analysis_name": "Nijmegen",
            "model_name": "_Nijmegen_only",
        },
        {
            "analysis_name": "Delta",
            "model_name": "_Delta_band",
        },
        {
            "analysis_name": "Theta",
            "model_name": "_Theta_band",
        },
        {
            "analysis_name": "Alpha",
            "model_name": "_Alpha_band",
        },
        {
            "analysis_name": "Theta_Alpha",
            "model_name": "_Theta_Alpha_bands",
        },
        {
            "analysis_name": "Beta",
            "model_name": "_Beta_band",
        },
        {
            "analysis_name": "Gamma",
            "model_name": "_Gamma_band",
        },
        {
            "analysis_name": "Full",
            "model_name": "_Full_band",
        },
    ]

    # Loop through each analysis and run the main function to train and test the GNN model
    for analysis in analyses:
        print(f"\n=== Running analysis: {analysis['analysis_name']} ===")

        # Set the directory for this analysis
        model_name = analysis["model_name"]

        # Set the name of the analysis
        analysis_name = analysis['analysis_name']

        # Get the corresponding files for the analysis
        fif_folder, scout_folder = fix_raw_files(analysis['analysis_name'], input_type=input_type)

        # Run the main function with the specified running parameters
        main(run_state, analysis_name, model_name, generate_plots, explainability_analysis, 
             input_type=input_type, fif_folder=fif_folder, scout_folder=scout_folder)
    
    # If a cross-analysis is to be performed, run the saved model on new data
    if run_cross_analysis:
        run_saved_model_on_new_data(
            new_data_analysis=new_data_analysis,
            model_name=cross_model_name,
            input_type=input_type,
            generate_plots=False,
            explainability_analysis=True
        )